多项式扩展 + LR
    原始
        特征属性: x1、x2、x3
        目标属性: y
        LR预测函数: p = sigmoid(w1*x1+w2*x2+w3*x3)
    二阶的多项式扩展:
        特征属性:
            x1、x2、x3、x1*x2、x1*x3、x2*x3、x1^2、x2^2、x3^2
        目标属性: y
        LR预测函数: p = sigmoid(
                w1*x1 + w2*x2 + w3*x3 +
                w4*x1*x2 + w5*x1*x3 + w6*x2*x3 +
                w7*x1^2 + w8*x2^2 + w9*x3^2
            )
        多项式扩展相当于将坐标系做了一个转换，是从低维到高维的一个映射
        多项式扩展就相当于是一种映射函数(理解成神经元的功能)
            z1=f1(x1,x2,x3)=x1
            z2=f2(x1,x2,x3)=x2
            z3=f3(x1,x2,x3)=x3
            z4=f4(x1,x2,x3)=x1*x2
            z5=f5(x1,x2,x3)=x1*x3
            z6=f6(x1,x2,x3)=x2*x3
            z7=f7(x1,x2,x3)=x1^2
            z8=f8(x1,x2,x3)=x2^2
            z9=f9(x1,x2,x3)=x3^2
    NOTE: 多项式扩展是不是就是将原始的特征属性(x1,x2,x3); 通过映射函数(神经元)映射到(z1,z2,z3,z4,z5,z6,z7,z8,z9)后，就会发现原来线性不可分的数据变成了线性可分。
        eg:
            (1,5,3) -> (1,5,3,5,3,15,1,25,9)
    深度学习：
        个人认为深度学习/神经网络是LR(线性转换+激活函数)的堆叠
        原始数据:
            x1,x2
            p=sigmoid(w1*x1+w2*x2+b)
        二阶多项式扩展数据:
            x1,x2,x1^2、x2^2
            p=sigmoid(w1*x1 + w2*x2 + w3*x1^2 + w4*x2^2 + b)
                z1=f1(x1,x2)=x1
                z2=f2(x1,x2)=x2
                z3=f3(x1,x2)=x1^2
                z4=f4(x1,x2)=x2^2
                p = sigmoid(w1*z1 + w2*z2 + w3*z3 + w4*z4 + b)
        深度学习数据:
            x1,x2 --> z1,z2,z3 --> p
                z1=f1(x1,x2)=tanh(-1.2*x1-0.13*x2-2.0)
                z2=f2(x1,x2)=tanh(-0.58*x1-1.3*x2+3.0)
                z3=f3(x1,x2)=tanh(0.82*x1-1.1*x2-3.0)
                p = sigmoid(-1.7*z1 + 1.8*z2 + -1.8*z3 + 0.0)
            每个神经元的功能就是提取特征信息，对每个样本而言每个神经元(每组参数)相当于从某个方面进行特征的描述/特征的提取；至于每个神经元具体提取什么特征由参数决定，参数又由当前模型的训练数据决定(损失最小化)
        NOTE:
            如果没有激活函数的时候，不管深度学习的网络有多少层，都相当于单层的线性模型
                p = sigmoid(-1.7*z1 + 1.8*z2 + -1.8*z3 + 0.0)
                    = sigmoid(
                            -1.7*(-1.2*x1-0.13*x2-2.0) +
                            1.8*(-0.58*x1-1.3*x2+3.0) +
                            -1.8*(0.82*x1-1.1*x2-3.0) +
                            0.0
                        )
                    = sigmoid(-0.48 * x1 + -0.139 * x2 + 14.2)

机器学习模型训练代码结构：
    1. 数据加载: 从磁盘或者其他位置加载数据到内存
    2. 数据处理：
    3. 特征工程：哑编码、标准化.....
    4. 模型训练
        4.1 创建 --> 创建一个新的算法模型对象即可
        4.2 训练 --> fit方法调用即可
    5. 模型评估
        5.1 直接调用封装好的方法即可
    6. 模型持久化保存磁盘
机器学习模型推理应用代码结构:
    1. 加载恢复模型(结构 + 参数，NOTE: 模型持久化的方式和模型恢复的方式必须是一一对应的)
    2. 和训练采用相同的流程，对待预测的数据进行处理转换
    3. 调用模型的predict方法(预测方法)获取得到预测结果
    4. 后处理转换 --> 在模型预测结果的基础上额外的进行一些数据处理的工作(调用方需要的相关逻辑结构)
=========================================================
深度学习模型训练代码结构：
    1. 数据加载: 将数据转换为Tensor对象-->构造一个DataSet和DataLoader即可
    2. 数据处理
    3. 模型训练
        3.1 创建 --> 需要人为构造出网络结构、优化器、损失函数
            模型初始化 ----> 构造网络的执行图(构建图中的各个模块)
            Loss Function的构造
            优化器构造
        3.2 训练 --> 需要人为进行数据的遍历以及前向反向过程的代码编写
            3.2.1 前向过程的执行 ---->
                属于网络的执行图的构建(模型的执行顺序)
                loss的获取
            3.2.2 反向过程的执行 ----> 不需要人为构造（框架会帮我们完成）
                + 梯度计算 + 参数的更新 + 梯度重置为0
            NOTE: 训练是一个循环的过程，所以在训练过程中会有模型评估和模型持久化的操作
    4. 模型评估
        4.1 需要人为进行数据遍历、模型的推理预测、预测结果的评估
    5. 模型持久化保存磁盘
深度学习模型推理应用代码结构:
    1. 加载恢复模型(结构 + 参数，NOTE: 模型持久化的方式和模型恢复的方式必须是一一对应的)
    2. 和训练采用相同的流程，对待预测的数据进行处理转换
    3. 调用模型的预测方法(前向过程)获取得到预测结果
    4. 后处理转换 --> 在模型预测结果的基础上额外的进行一些数据处理的工作
