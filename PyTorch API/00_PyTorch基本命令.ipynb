{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-10T13:59:04.512657Z",
     "start_time": "2025-12-10T13:59:02.526146Z"
    }
   },
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:59:07.874874Z",
     "start_time": "2025-12-10T13:59:07.870163Z"
    }
   },
   "cell_type": "code",
   "source": "torch.__version__",
   "id": "53854a54b8862cb8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.3.1+cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T02:57:41.799775Z",
     "start_time": "2025-12-10T02:57:41.796995Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PyTorch 类似 NumPy 框架\n",
    "# NumPy 框架本身是一个数值运算(数学运算)的一个框架 --> 内部最核心的 ndarray --> 数组/矩阵\n",
    "## NumPy常见的操作：创建、乘法、加法、指数、对数、均值、和......\n",
    "\n",
    "## PyTorch 类似 sklearn 的模型学习的框架，基本操作类似Numpy语法\n",
    "### 关注PyTorch中的网络的构造(算法结构)、损失函数、优化器"
   ],
   "id": "8633667971aa6495",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T02:57:42.143173Z",
     "start_time": "2025-12-10T02:57:42.140088Z"
    }
   },
   "cell_type": "code",
   "source": "## 重点关注Tensor的创建、模型的创建、损失的创建、优化器的使用",
   "id": "a1ce318619017474",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Tensor创建\n",
    "Tensor对象是PyTorch中的基本对象，一般用来表示不需要进行参数更新的数据对象"
   ],
   "id": "8da4e52e7080c285"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 1. 随机创建Tensor对象",
   "id": "89de96f02c7da36"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:44:00.092964Z",
     "start_time": "2025-12-10T13:44:00.087098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## PS: PyTorch中随机创建Tensor默认浮点型为float32，模型整型为int64；默认情况下数据所在设备均在CPU；\n",
    "## 可以通过dtype给定数据类型、device给定运行设备\n",
    "x = torch.randn(2, 5)  # 随机创建一个[2,5]的矩阵，满足X~N(0,1)的标准正态分布随机数\n",
    "# x = torch.randn(2, 5, dtype=torch.float64)  # 随机创建一个[2,5]的矩阵，满足X~N(0,1)的标准正态分布随机数\n",
    "print(x)\n",
    "print(f\"Shape形状为: {x.shape}\")\n",
    "print(f\"数据的类型: {x.dtype}\")\n",
    "print(f\"数据所在设备信息: {x.device}\")\n",
    "print(f\"是否属于梯度更新参数: {x.requires_grad}\")"
   ],
   "id": "3e5bd841404dd8d9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0678, -1.2220, -0.5441,  1.2008, -0.7029],\n",
      "        [ 0.6055,  0.5489,  0.7728, -1.5179, -0.1129]])\n",
      "Shape形状为: torch.Size([2, 5])\n",
      "数据的类型: torch.float32\n",
      "数据所在设备信息: cpu\n",
      "是否属于梯度更新参数: False\n"
     ]
    }
   ],
   "execution_count": 112
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:44:07.005782Z",
     "start_time": "2025-12-10T13:44:06.999542Z"
    }
   },
   "cell_type": "code",
   "source": "torch.randint(0, 10, size=(10,)).dtype",
   "id": "b3b1e1a15bf95776",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:45:05.752813Z",
     "start_time": "2025-12-10T13:45:05.747431Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.ones((2,3), dtype=torch.float64))",
   "id": "a85471a9009dd62",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 116
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:45:13.056289Z",
     "start_time": "2025-12-10T13:45:13.050162Z"
    }
   },
   "cell_type": "code",
   "source": "print(torch.zeros((2,3)))",
   "id": "7215b4c0f3727dc7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "execution_count": 117
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### 2. 基于已有数据创建Tensor对象",
   "id": "c807785a5097d43f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:47:28.408505Z",
     "start_time": "2025-12-10T13:47:28.401044Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## PS: 基于python类型数据可创建tensor对象，一般支持数字或者list数组；\n",
    "## 创建时候默认会自动推断数据类型，浮点型为float32，整型为int64，设备默认为CPU\n",
    "## 可以通过dtype给定数据类型、device给定运行设备\n",
    "## NOTE: Tensor底层和python数据类型不共享内存，相当于是一个副本，更改互不影响\n",
    "x_py = [1,2,3,4,5,6]\n",
    "# x_py = 2\n",
    "x = torch.tensor(x_py)  # python对象转Tensor对象\n",
    "#x = torch.tensor(x_py, dtype=torch.int32)\n",
    "xtolst = x.tolist()  # Tensor对象转换为list --> 仅在cpu设备上的tensor进行转换\n",
    "\n",
    "print(x)\n",
    "print(f\"Shape形状为: {x.shape}\")\n",
    "print(f\"数据的类型: {x.dtype}\")\n",
    "print(f\"数据所在设备信息: {x.device}\")\n",
    "print(f\"是否属于梯度更新参数: {x.requires_grad}\")\n",
    "if isinstance(x_py, list):\n",
    "    x_py[0] = 100\n",
    "    x[1] = 101\n",
    "    print(f\"更改后Python对象为: {x_py}\")\n",
    "    print(f\"更改后Tensor对象为: {x}\")\n",
    "print(f\"Tensor转换的Python列表对象为: {type(xtolst)} - {xtolst}\")\n"
   ],
   "id": "aa5d933c18513279",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6])\n",
      "Shape形状为: torch.Size([6])\n",
      "数据的类型: torch.int64\n",
      "数据所在设备信息: cpu\n",
      "是否属于梯度更新参数: False\n",
      "更改后Python对象为: [100, 2, 3, 4, 5, 6]\n",
      "更改后Tensor对象为: tensor([  1, 101,   3,   4,   5,   6])\n",
      "Tensor转换的Python列表对象为: <class 'list'> - [1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "execution_count": 119
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:49:54.395682Z",
     "start_time": "2025-12-10T13:49:54.390400Z"
    }
   },
   "cell_type": "code",
   "source": [
    "## PS: 基于Numpy类型数据可创建tensor对象，一般支持数字或者数组；\n",
    "## 创建时候类型默认和所给定的Numpy类型一致，设备默认为CPU\n",
    "## torch.tensor：可以通过dtype给定数据类型、device给定运行设备\n",
    "x_np = np.asarray([1, 2, 3, 4, 5, 6])\n",
    "# x = torch.tensor(x_np)  # Numpy对象转Tensor对象  不共享内存\n",
    "x = torch.from_numpy(x_np)  # 直接基于Numpy对象的内存进行Tensor对象创建 -- 共享内存\n",
    "xtonp = x.numpy()  # Tensor对象转换为Numpy对象（共享内存） --> 仅在cpu设备上的tensor进行转换\n",
    "\n",
    "print(x)\n",
    "print(f\"Shape形状为: {x.shape}\")\n",
    "print(f\"数据的类型: {x.dtype}\")\n",
    "print(f\"数据所在设备信息: {x.device}\")\n",
    "print(f\"是否属于梯度更新参数: {x.requires_grad}\")\n",
    "x_np[0] = 100\n",
    "x[1] = 101\n",
    "print(f\"更改后NumPy对象为: {x_np}\")\n",
    "print(f\"更改后Tensor对象为: {x}\")\n",
    "print(f\"Tensor转换的NumPy列表对象为: {type(xtonp)} - {xtonp}\")\n"
   ],
   "id": "f383b63567692614",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3, 4, 5, 6], dtype=torch.int32)\n",
      "Shape形状为: torch.Size([6])\n",
      "数据的类型: torch.int32\n",
      "数据所在设备信息: cpu\n",
      "是否属于梯度更新参数: False\n",
      "更改后NumPy对象为: [100 101   3   4   5   6]\n",
      "更改后Tensor对象为: tensor([100, 101,   3,   4,   5,   6], dtype=torch.int32)\n",
      "Tensor转换的NumPy列表对象为: <class 'numpy.ndarray'> - [100 101   3   4   5   6]\n"
     ]
    }
   ],
   "execution_count": 122
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Tensor设备、类型、Shape转换",
   "id": "f6613fe8582f8422"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:59:19.730836Z",
     "start_time": "2025-12-10T13:59:19.674411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "x = torch.randn(3, 6)\n",
    "print(f\"Shape形状为: {x.shape}\")\n",
    "print(f\"数据的类型: {x.dtype}\")\n",
    "print(f\"数据所在设备信息: {x.device}\")\n",
    "print(f\"是否属于梯度更新参数: {x.requires_grad}\")\n",
    "print(x)"
   ],
   "id": "78f34225a57e9cbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape形状为: torch.Size([3, 6])\n",
      "数据的类型: torch.float32\n",
      "数据所在设备信息: cpu\n",
      "是否属于梯度更新参数: False\n",
      "tensor([[ 1.8837, -0.9789,  0.0743, -1.0235, -0.7356,  0.6972],\n",
      "        [-2.4609, -0.7360, -1.0982,  0.1817,  0.1530,  0.7673],\n",
      "        [-0.8827, -0.2661, -2.0446,  1.1278, -0.5159,  1.2721]])\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:59:20.625707Z",
     "start_time": "2025-12-10T13:59:20.619658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "_x = torch.reshape(x, shape=(3, 2, -1))\n",
    "#_x = x.reshape(shape=(3, 2, -1))\n",
    "print(\"torch.reshape后结果为:\")\n",
    "print(f\"Shape形状为: {_x.shape}\")\n",
    "print(f\"数据的类型: {_x.dtype}\")\n",
    "print(f\"数据所在设备信息: {_x.device}\")\n",
    "print(f\"是否属于梯度更新参数: {_x.requires_grad}\")\n",
    "print(_x)"
   ],
   "id": "c689afdc53814bf4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.reshape后结果为:\n",
      "Shape形状为: torch.Size([3, 2, 3])\n",
      "数据的类型: torch.float32\n",
      "数据所在设备信息: cpu\n",
      "是否属于梯度更新参数: False\n",
      "tensor([[[ 1.8837, -0.9789,  0.0743],\n",
      "         [-1.0235, -0.7356,  0.6972]],\n",
      "\n",
      "        [[-2.4609, -0.7360, -1.0982],\n",
      "         [ 0.1817,  0.1530,  0.7673]],\n",
      "\n",
      "        [[-0.8827, -0.2661, -2.0446],\n",
      "         [ 1.1278, -0.5159,  1.2721]]])\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:59:21.199391Z",
     "start_time": "2025-12-10T13:59:21.194918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 从效果上来讲，view和reshape都是更改tensor对象的shape形状，但是执行效率和内部逻辑是不一样的，view的效果更高一些，但是view要求tensor对象在内存中必须是连续空间上的值\n",
    "_x = x.view(2, -1)\n",
    "# _x = x.T.view(2, -1) # x.T x转置后，数据是不连续的，此时不可以使用view方法\n",
    "print(\"Tensor.view后结果为:\")\n",
    "print(f\"Shape形状为: {_x.shape}\")\n",
    "print(f\"数据的类型: {_x.dtype}\")\n",
    "print(f\"数据所在设备信息: {_x.device}\")\n",
    "print(f\"是否属于梯度更新参数: {_x.requires_grad}\")\n",
    "print(_x)"
   ],
   "id": "1b4acb1f710f8de8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor.view后结果为:\n",
      "Shape形状为: torch.Size([2, 9])\n",
      "数据的类型: torch.float32\n",
      "数据所在设备信息: cpu\n",
      "是否属于梯度更新参数: False\n",
      "tensor([[ 1.8837, -0.9789,  0.0743, -1.0235, -0.7356,  0.6972, -2.4609, -0.7360,\n",
      "         -1.0982],\n",
      "        [ 0.1817,  0.1530,  0.7673, -0.8827, -0.2661, -2.0446,  1.1278, -0.5159,\n",
      "          1.2721]])\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:59:22.317387Z",
     "start_time": "2025-12-10T13:59:22.310627Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用to命令进行类型或者设备信息的转换\n",
    "_x = x.to(dtype=torch.float64)\n",
    "print(\"to方法转换数据类型后:\")\n",
    "print(f\"Shape形状为: {_x.shape}\")\n",
    "print(f\"数据的类型: {_x.dtype}\")\n",
    "print(f\"数据所在设备信息: {_x.device}\")\n",
    "print(f\"是否属于梯度更新参数: {_x.requires_grad}\")\n",
    "print(_x)"
   ],
   "id": "b542255d6cb0c14a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to方法转换数据类型后:\n",
      "Shape形状为: torch.Size([3, 6])\n",
      "数据的类型: torch.float64\n",
      "数据所在设备信息: cpu\n",
      "是否属于梯度更新参数: False\n",
      "tensor([[ 1.8837, -0.9789,  0.0743, -1.0235, -0.7356,  0.6972],\n",
      "        [-2.4609, -0.7360, -1.0982,  0.1817,  0.1530,  0.7673],\n",
      "        [-0.8827, -0.2661, -2.0446,  1.1278, -0.5159,  1.2721]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T13:59:55.857472Z",
     "start_time": "2025-12-10T13:59:55.846801Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 当支持GPU的时候，使用cuda，否则使用cpu\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"当前设备为:{device}\")\n",
    "_x = x.to(device=device)\n",
    "print(\"to方法转换设备信息后:\")\n",
    "print(f\"Shape形状为: {_x.shape}\")\n",
    "print(f\"数据的类型: {_x.dtype}\")\n",
    "print(f\"数据所在设备信息: {_x.device}\")\n",
    "print(f\"是否属于梯度更新参数: {_x.requires_grad}\")\n",
    "print(_x)\n",
    "print(f\"快捷转换到cpu方式:\\n{_x.cpu()}\")"
   ],
   "id": "1984190d9af9d03d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前设备为:cpu\n",
      "to方法转换设备信息后:\n",
      "Shape形状为: torch.Size([3, 6])\n",
      "数据的类型: torch.float32\n",
      "数据所在设备信息: cpu\n",
      "是否属于梯度更新参数: False\n",
      "tensor([[ 1.8837, -0.9789,  0.0743, -1.0235, -0.7356,  0.6972],\n",
      "        [-2.4609, -0.7360, -1.0982,  0.1817,  0.1530,  0.7673],\n",
      "        [-0.8827, -0.2661, -2.0446,  1.1278, -0.5159,  1.2721]])\n",
      "快捷转换到cpu方式:\n",
      "tensor([[ 1.8837, -0.9789,  0.0743, -1.0235, -0.7356,  0.6972],\n",
      "        [-2.4609, -0.7360, -1.0982,  0.1817,  0.1530,  0.7673],\n",
      "        [-0.8827, -0.2661, -2.0446,  1.1278, -0.5159,  1.2721]])\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Tensor常见数值运算API\n",
    "基本操作基本和NumPy里面的数值操作解决，主要区别就是：多tensor之间进行操作的时候，要求必须数据类型、数据所在设备信息均一样；某些封装好的特殊API除外。"
   ],
   "id": "dd05a1b42e10fa7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:02:18.966433Z",
     "start_time": "2025-12-10T03:02:18.961119Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 加法\n",
    "x = torch.randn(2, 3)\n",
    "#y = 5\n",
    "# y = torch.rand(3)\n",
    "#y = torch.rand(2, 1)\n",
    "y = torch.rand(2, 3)\n",
    "print(x)\n",
    "print(y)\n",
    "print(x + y)"
   ],
   "id": "4fb50859bb22845b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5144, -0.2252, -0.3888],\n",
      "        [ 0.9117, -1.2421, -0.7164]])\n",
      "tensor([[0.1881, 0.7752, 0.1511],\n",
      "        [0.4797, 0.7631, 0.6294]])\n",
      "tensor([[ 0.7025,  0.5500, -0.2377],\n",
      "        [ 1.3913, -0.4790, -0.0870]])\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:05:57.514359Z",
     "start_time": "2025-12-10T03:05:57.509568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 矩阵乘法\n",
    "x = torch.rand(2, 3)\n",
    "y = torch.randn(3, 2)\n",
    "print(x @ y)\n",
    "print(torch.matmul(x, y))"
   ],
   "id": "da583d27ac4487eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9547, 0.0513],\n",
      "        [1.9657, 0.2050]])\n",
      "tensor([[0.9547, 0.0513],\n",
      "        [1.9657, 0.2050]])\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:06:44.050273Z",
     "start_time": "2025-12-10T03:06:44.034020Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 多维矩阵乘法\n",
    "# 支持广播机制，实际上只有最后两个维度参与矩阵乘法，其它维度相当于遍历\n",
    "x = torch.randn(2, 3, 2)\n",
    "y = torch.randn(2, 2, 1)\n",
    "\n",
    "print(x @ y)\n",
    "print(torch.matmul(x, y))\n",
    "\n",
    "for i in range(2):\n",
    "    print(x[i] @ y[i])"
   ],
   "id": "c1d496cf799f8a01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.5291],\n",
      "         [ 0.6880],\n",
      "         [ 1.1998]],\n",
      "\n",
      "        [[ 0.5508],\n",
      "         [ 0.1603],\n",
      "         [ 0.0309]]])\n",
      "tensor([[[-0.5291],\n",
      "         [ 0.6880],\n",
      "         [ 1.1998]],\n",
      "\n",
      "        [[ 0.5508],\n",
      "         [ 0.1603],\n",
      "         [ 0.0309]]])\n",
      "tensor([[-0.5291],\n",
      "        [ 0.6880],\n",
      "        [ 1.1998]])\n",
      "tensor([[0.5508],\n",
      "        [0.1603],\n",
      "        [0.0309]])\n"
     ]
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:10:20.543196Z",
     "start_time": "2025-12-10T03:10:20.535335Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 求均值\n",
    "x = torch.randn(2, 3, 5)\n",
    "print(torch.mean(x, dim=1))  # 按照维度1进行数据均值求解\n",
    "print(x.mean(dim=1))  # 按照维度1进行数据均值求解\n",
    "print((x[:, 0, :] + x[:, 1, :] + x[:, 2]) / 3.0)"
   ],
   "id": "7dc37749987bb9c2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0238,  0.2865,  0.5136,  0.2461, -0.4586],\n",
      "        [-0.4214, -0.6863, -0.7808, -0.4715,  0.0568]])\n",
      "tensor([[-0.0238,  0.2865,  0.5136,  0.2461, -0.4586],\n",
      "        [-0.4214, -0.6863, -0.7808, -0.4715,  0.0568]])\n",
      "tensor([[-0.0238,  0.2865,  0.5136,  0.2461, -0.4586],\n",
      "        [-0.4214, -0.6863, -0.7808, -0.4715,  0.0568]])\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:12:32.958431Z",
     "start_time": "2025-12-10T03:12:32.953378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 求和\n",
    "x = torch.randn(2, 3, 5)\n",
    "print(x.sum(dim=2))  # 按照维度-1也就是维度2 进行数据和求解\n",
    "print(x[..., 0] + x[..., 1] + x[:, :, 2] + x[:, :, 3] + x[:, :, 4])"
   ],
   "id": "c46af4a11674e1bf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3973,  4.8920, -1.4553],\n",
      "        [ 5.1417,  3.2435,  1.2985]])\n",
      "tensor([[ 0.3973,  4.8920, -1.4553],\n",
      "        [ 5.1417,  3.2435,  1.2985]])\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:13:00.751691Z",
     "start_time": "2025-12-10T03:13:00.746464Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 均值对\n",
    "x = torch.randn(5)\n",
    "print(x)\n",
    "print(torch.abs(x))"
   ],
   "id": "9e7774794c432423",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-1.5715,  0.3795,  2.1378, -0.6212, -0.9298])\n",
      "tensor([1.5715, 0.3795, 2.1378, 0.6212, 0.9298])\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:13:46.568430Z",
     "start_time": "2025-12-10T03:13:46.563208Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 最大数据做大的索引下标\n",
    "x = torch.randn(2, 5)\n",
    "print(x)\n",
    "print(torch.argmax(x, dim=-1)) # 在最后一个维度中选择值最大的对应下标"
   ],
   "id": "32417a927180a444",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1428,  0.4531,  2.6241, -0.4645,  1.4920],\n",
      "        [ 0.5055, -0.2768, -0.8597,  0.0029, -0.1022]])\n",
      "tensor([2, 0])\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:14:31.695664Z",
     "start_time": "2025-12-10T03:14:31.671982Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# top k：选择前k个比较大的值以及对应的索引id\n",
    "x = torch.randn(2, 5)\n",
    "print(x)\n",
    "print(torch.topk(x, k=3, dim=-1))"
   ],
   "id": "be767aba2cdbf935",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.2148, -0.3187, -0.4539,  0.6437, -1.2966],\n",
      "        [-0.8937, -0.1577, -0.5888, -0.5961,  0.2672]])\n",
      "torch.return_types.topk(\n",
      "values=tensor([[ 0.6437, -0.3187, -0.4539],\n",
      "        [ 0.2672, -0.1577, -0.5888]]),\n",
      "indices=tensor([[3, 1, 2],\n",
      "        [4, 1, 2]]))\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:15:40.099775Z",
     "start_time": "2025-12-10T03:15:40.089938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据转置\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(x.T)\n",
    "print(torch.transpose(x, dim0=1, dim1=0))"
   ],
   "id": "6a82772b767655f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2419,  0.6105,  0.5217],\n",
      "        [ 0.0726,  0.7789, -0.0131]])\n",
      "tensor([[-1.2419,  0.0726],\n",
      "        [ 0.6105,  0.7789],\n",
      "        [ 0.5217, -0.0131]])\n",
      "tensor([[-1.2419,  0.0726],\n",
      "        [ 0.6105,  0.7789],\n",
      "        [ 0.5217, -0.0131]])\n"
     ]
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:18:36.291338Z",
     "start_time": "2025-12-10T03:18:36.288301Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 多维数据交换 对应numpy接口为: np.transpose\n",
    "x = torch.randn(5, 3, 4, 10)\n",
    "print(x.shape)\n",
    "print(torch.permute(x, dims=(0, 3, 1, 2)).shape)"
   ],
   "id": "e5feca9e6ca657f8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 3, 4, 10])\n",
      "torch.Size([5, 10, 3, 4])\n"
     ]
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:20:56.529458Z",
     "start_time": "2025-12-10T03:20:56.522929Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tensor对象分割\n",
    "x = torch.randn(2,6)\n",
    "print(x)\n",
    "# split中split_size_or_sections是给定分割后每个tensor对应维度的大小\n",
    "print(torch.split(x, split_size_or_sections=3, dim=1))\n",
    "# chunk中的chunks是预计将对应维度分割成几份\n",
    "print(torch.chunk(x, chunks=3, dim=1))"
   ],
   "id": "a0dd415d101d6e7a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7699, -0.9905, -1.1317, -0.5456,  3.3501, -0.1544],\n",
      "        [ 1.1470, -0.5987, -0.4692,  0.1179,  0.1022, -1.4465]])\n",
      "(tensor([[-1.7699, -0.9905, -1.1317],\n",
      "        [ 1.1470, -0.5987, -0.4692]]), tensor([[-0.5456,  3.3501, -0.1544],\n",
      "        [ 0.1179,  0.1022, -1.4465]]))\n",
      "(tensor([[-1.7699, -0.9905],\n",
      "        [ 1.1470, -0.5987]]), tensor([[-1.1317, -0.5456],\n",
      "        [-0.4692,  0.1179]]), tensor([[ 3.3501, -0.1544],\n",
      "        [ 0.1022, -1.4465]]))\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:21:45.193190Z",
     "start_time": "2025-12-10T03:21:45.173253Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 数据合并\n",
    "x1 = torch.rand(1,3)\n",
    "x2 = torch.rand(3,3)\n",
    "print(\"=\" * 10)\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(\"=\" * 10)\n",
    "print(torch.cat([x1, x2], dim=0))"
   ],
   "id": "c05709dbaaa0b55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========\n",
      "tensor([[0.0342, 0.4128, 0.0895]])\n",
      "tensor([[0.8726, 0.8803, 0.7500],\n",
      "        [0.6683, 0.6621, 0.1977],\n",
      "        [0.5890, 0.6204, 0.6089]])\n",
      "==========\n",
      "tensor([[0.0342, 0.4128, 0.0895],\n",
      "        [0.8726, 0.8803, 0.7500],\n",
      "        [0.6683, 0.6621, 0.1977],\n",
      "        [0.5890, 0.6204, 0.6089]])\n"
     ]
    }
   ],
   "execution_count": 62
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Parameter定义",
   "id": "e365d3b2450e2459"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:25:13.826965Z",
     "start_time": "2025-12-10T03:25:13.822270Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w_tensor = torch.randn(2, 3)\n",
    "# 除了 requires_grad 信息默认更改为True，其它的相当于没有变化\n",
    "## requires_grad 用来标识是否需要进行参数更新(在反向传播中，会自动针对所有requires_grad为True的对象进行对应梯度的计算)\n",
    "w_param = nn.Parameter(w_tensor)\n",
    "print(f\"Shape形状为: {w_tensor.shape} - {w_param.shape}\")\n",
    "print(f\"数据的类型: {w_tensor.dtype} - {w_param.dtype}\")\n",
    "print(f\"数据所在设备信息: {w_tensor.device} - {w_param.device}\")\n",
    "print(f\"是否属于梯度更新参数: {w_tensor.requires_grad} - {w_param.requires_grad}\")\n",
    "print(w_tensor)\n",
    "print(w_param)"
   ],
   "id": "776e786eb65a2350",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape形状为: torch.Size([2, 3]) - torch.Size([2, 3])\n",
      "数据的类型: torch.float32 - torch.float32\n",
      "数据所在设备信息: cpu - cpu\n",
      "是否属于梯度更新参数: False - True\n",
      "tensor([[ 1.2765, -0.4361,  0.5606],\n",
      "        [-1.9726,  2.3841, -1.1659]])\n",
      "Parameter containing:\n",
      "tensor([[ 1.2765, -0.4361,  0.5606],\n",
      "        [-1.9726,  2.3841, -1.1659]], requires_grad=True)\n"
     ]
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:25:36.142088Z",
     "start_time": "2025-12-10T03:25:36.114901Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = nn.Parameter(torch.randn(2, 3))\n",
    "x = torch.randn(1, 2)\n",
    "# 当前序的执行节点上存在需要计算梯度的情况下，PyTorch默认会自动在当前算子上增加一个grad_fn用来计算相关梯度\n",
    "print(x @ w)"
   ],
   "id": "8f3ac2285fe0b7e1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.0682, -1.3473,  0.2598]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:27:53.397547Z",
     "start_time": "2025-12-10T03:27:53.391809Z"
    }
   },
   "cell_type": "code",
   "source": [
    "w = nn.Parameter(torch.randn(2, 3))\n",
    "x = torch.randn(1, 2)\n",
    "print(x @ w) # 会有求梯度的函数自动添加\n",
    "with torch.no_grad():\n",
    "    # 当不需要计算梯度的时候，可以考虑使用np_grad 或者 detach\n",
    "    print(x @ w) # 不会有梯度传递了\n",
    "print((x @ w).detach()) # 不会有梯度传递了"
   ],
   "id": "d14053eb5a3065b8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.1150, -0.8742,  0.1726]], grad_fn=<MmBackward0>)\n",
      "tensor([[-1.1150, -0.8742,  0.1726]])\n",
      "tensor([[-1.1150, -0.8742,  0.1726]])\n"
     ]
    }
   ],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### 最基本的模块",
   "id": "899f39e3c3709153"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:30:52.617392Z",
     "start_time": "2025-12-10T03:30:52.602362Z"
    }
   },
   "cell_type": "code",
   "source": [
    "linear = nn.Linear(in_features=2, out_features=3, bias=True) # 创建模块\n",
    "for name, param in linear.named_parameters():\n",
    "    print(f\"参数名称: {name} -- 参数为:\\n{param}\\n\")\n",
    "#x = torch.randn(3, 1, 2) # 当前模块的输入数据， 支持多维数据的输入\n",
    "x = torch.randn(1, 2) # 当前模块的输入数据， 支持多维数据的输入\n",
    "r = linear(x) # 调用模块内部执行的方法\n",
    "print(r)\n",
    "print(x @ linear.weight.T + linear.bias) # Linear等价"
   ],
   "id": "71602e8a74d1d47d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "参数名称: weight -- 参数为:\n",
      "Parameter containing:\n",
      "tensor([[-0.4570,  0.2157],\n",
      "        [ 0.6735, -0.0248],\n",
      "        [ 0.5459, -0.6490]], requires_grad=True)\n",
      "\n",
      "参数名称: bias -- 参数为:\n",
      "Parameter containing:\n",
      "tensor([-0.1535,  0.3038, -0.1105], requires_grad=True)\n",
      "\n",
      "tensor([[[-0.7925,  0.7259,  1.3464]],\n",
      "\n",
      "        [[ 0.2429, -0.2416, -0.6356]],\n",
      "\n",
      "        [[ 0.1576, -0.2699, -0.3283]]], grad_fn=<ViewBackward0>)\n",
      "tensor([[[-0.7925,  0.7259,  1.3464]],\n",
      "\n",
      "        [[ 0.2429, -0.2416, -0.6356]],\n",
      "\n",
      "        [[ 0.1576, -0.2699, -0.3283]]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:31:34.400267Z",
     "start_time": "2025-12-10T03:31:34.379297Z"
    }
   },
   "cell_type": "code",
   "source": [
    "relu = nn.ReLU() # 激活模块\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(relu(x))"
   ],
   "id": "958fab0073e17050",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5360,  1.6281, -0.4169],\n",
      "        [-0.1435,  0.8949, -0.2826]])\n",
      "tensor([[0.5360, 1.6281, 0.0000],\n",
      "        [0.0000, 0.8949, 0.0000]])\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:31:47.906745Z",
     "start_time": "2025-12-10T03:31:47.890763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sigmoid = nn.Sigmoid() # 激活模块\n",
    "x = torch.randn(2, 3)\n",
    "print(x)\n",
    "print(sigmoid(x))"
   ],
   "id": "b2627a5931f963f0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.5775,  0.1776,  1.0715],\n",
      "        [-1.0094,  1.1908, -0.1650]])\n",
      "tensor([[0.3595, 0.5443, 0.7449],\n",
      "        [0.2671, 0.7669, 0.4589]])\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T03:34:00.203495Z",
     "start_time": "2025-12-10T03:34:00.198502Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# PyTorch封装好的序列模块，主要用来封装整个串行执行链路\n",
    "## PS: 不允许使用普通python的集合列表对象，PyTorch无法识别\n",
    "seq = nn.Sequential(\n",
    "    nn.Linear(2, 3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(3, 1)\n",
    ")\n",
    "for ch in seq.children():\n",
    "    print(f\"子模块 - {ch}\")\n",
    "x = torch.randn(2, 2)\n",
    "r = seq(x)\n",
    "print(r.shape)\n",
    "print(r)"
   ],
   "id": "8c838c5eae387814",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "子模块 - Linear(in_features=2, out_features=3, bias=True)\n",
      "子模块 - ReLU()\n",
      "子模块 - Linear(in_features=3, out_features=1, bias=True)\n",
      "torch.Size([2, 1])\n",
      "tensor([[-0.2024],\n",
      "        [-0.3114]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "execution_count": 81
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Softmax交叉熵损失函数",
   "id": "450ff69da705c5d4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T07:12:45.553783Z",
     "start_time": "2025-12-10T07:12:45.547463Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "score = torch.randn(4, 5) # 4个样本，每个样本属于5个类别的置信度 -- 一般来源于网络的输出\n",
    "target = torch.tensor([1, 3, 0, 0]) # 4个样本，每个样本对应的类别id 取值范围[0,5)+和类别数目有关\n",
    "\n",
    "loss = loss_fn(score, target)\n",
    "loss"
   ],
   "id": "1199e0e8f904f14e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.0754)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 88
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T07:12:47.052694Z",
     "start_time": "2025-12-10T07:12:47.047017Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义损失函数\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "linear = nn.Linear(3, 5)\n",
    "\n",
    "x = torch.randn(4, 3)\n",
    "score = linear(x) # 4个样本，每个样本属于5个类别的置信度 -- 一般来源于网络的输出\n",
    "target = torch.tensor([1, 3, 0, 0]) # 4个样本，每个样本对应的类别id 取值范围[0,5)+和类别数目有关\n",
    "\n",
    "loss = loss_fn(score, target)\n",
    "loss"
   ],
   "id": "9b283fadab2c81b8",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5173, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 89
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T07:15:02.129293Z",
     "start_time": "2025-12-10T07:15:02.104846Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义损失函数\n",
    "loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "linear = nn.Linear(3, 5)\n",
    "\n",
    "x = torch.randn(4, 3)\n",
    "score = linear(x) # 4个样本，每个样本属于5个类别的置信度 -- 一般来源于网络的输出\n",
    "target = torch.tensor([1, 3, 0, 0]) # 4个样本，每个样本对应的类别id 取值范围[0,5)+和类别数目有关\n",
    "\n",
    "loss = loss_fn(score, target)\n",
    "print(f\"直接损失值:\\n{loss}\\n\")\n",
    "\n",
    "# 按照softmax概率公式求解\n",
    "proba = torch.softmax(score, dim=-1)\n",
    "print(target)\n",
    "print(proba)\n",
    "print(-torch.log(proba))\n",
    "# 获取对应位置\n",
    "print(-torch.log(proba)[[0,1,2,3], target])"
   ],
   "id": "93a15e8df1d47d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "直接损失值:\n",
      "tensor([2.2587, 1.0139, 1.8772, 1.0035], grad_fn=<NllLossBackward0>)\n",
      "\n",
      "tensor([1, 3, 0, 0])\n",
      "tensor([[0.5412, 0.1045, 0.1890, 0.1187, 0.0467],\n",
      "        [0.1279, 0.3859, 0.0705, 0.3628, 0.0529],\n",
      "        [0.1530, 0.1539, 0.2284, 0.2065, 0.2583],\n",
      "        [0.3666, 0.3098, 0.0913, 0.1250, 0.1072]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([[0.6139, 2.2587, 1.6662, 2.1315, 3.0647],\n",
      "        [2.0564, 0.9523, 2.6516, 1.0139, 2.9394],\n",
      "        [1.8772, 1.8718, 1.4768, 1.5775, 1.3537],\n",
      "        [1.0035, 1.1718, 2.3934, 2.0792, 2.2326]], grad_fn=<NegBackward0>)\n",
      "tensor([2.2587, 1.0139, 1.8772, 1.0035], grad_fn=<NegBackward0>)\n"
     ]
    }
   ],
   "execution_count": 96
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Sigmoid交叉熵损失函数",
   "id": "ee2e4b69b52137cd"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T07:18:16.656100Z",
     "start_time": "2025-12-10T07:18:16.649838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义损失函数\n",
    "loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "score = torch.randn(4, 5) # 4个样本，每个样本属于5个类别的置信度 -- 一般来源于网络的输出\n",
    "target = torch.tensor([\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0]\n",
    "], dtype=torch.float32) # 4个样本，属于当前类别，标签为1，不属于标签为0\n",
    "\n",
    "loss = loss_fn(score, target)\n",
    "loss"
   ],
   "id": "126d5edefe13ffe3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8207)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 103
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T07:18:27.195395Z",
     "start_time": "2025-12-10T07:18:27.189751Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 定义损失函数\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction='none')\n",
    "\n",
    "score = torch.randn(4, 5) # 4个样本，每个样本属于5个类别的置信度 -- 一般来源于网络的输出\n",
    "target = torch.tensor([\n",
    "    [1, 1, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 0]\n",
    "], dtype=torch.float32) # 4个样本，属于当前类别，标签为1，不属于标签为0\n",
    "\n",
    "loss = loss_fn(score, target)\n",
    "loss"
   ],
   "id": "3b987b1549e4cf87",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.4822, 0.0562, 0.2954, 0.8588, 1.1241],\n",
       "        [0.8763, 0.2808, 1.7110, 0.8424, 0.8038],\n",
       "        [0.7429, 0.2060, 0.1710, 1.4765, 0.5028],\n",
       "        [0.5495, 0.2177, 1.9942, 0.2941, 0.3373]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 104
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T07:19:37.228432Z",
     "start_time": "2025-12-10T07:19:37.222807Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 按照sigmoid概率公式求解\n",
    "proba = torch.sigmoid(score)\n",
    "print(target)\n",
    "print(proba)\n",
    "print(-torch.log(proba))\n",
    "print(-torch.log(1 - proba))\n",
    "print(loss)\n"
   ],
   "id": "de68ccd1e48edec4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 1., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 1., 0.]])\n",
      "tensor([[0.6174, 0.9454, 0.2557, 0.5763, 0.6750],\n",
      "        [0.5837, 0.2448, 0.8193, 0.4307, 0.5524],\n",
      "        [0.5243, 0.1862, 0.1572, 0.7716, 0.3951],\n",
      "        [0.5773, 0.1956, 0.8639, 0.7452, 0.2863]])\n",
      "tensor([[0.4822, 0.0562, 1.3636, 0.5511, 0.3930],\n",
      "        [0.5384, 1.4071, 0.1993, 0.8424, 0.5935],\n",
      "        [0.6457, 1.6810, 1.8505, 0.2593, 0.9285],\n",
      "        [0.5495, 1.6317, 0.1463, 0.2941, 1.2508]])\n",
      "tensor([[0.9609, 2.9068, 0.2954, 0.8588, 1.1241],\n",
      "        [0.8763, 0.2808, 1.7110, 0.5633, 0.8038],\n",
      "        [0.7429, 0.2060, 0.1710, 1.4765, 0.5028],\n",
      "        [0.8610, 0.2177, 1.9942, 1.3672, 0.3373]])\n",
      "tensor([[0.4822, 0.0562, 0.2954, 0.8588, 1.1241],\n",
      "        [0.8763, 0.2808, 1.7110, 0.8424, 0.8038],\n",
      "        [0.7429, 0.2060, 0.1710, 1.4765, 0.5028],\n",
      "        [0.5495, 0.2177, 1.9942, 0.2941, 0.3373]])\n"
     ]
    }
   ],
   "execution_count": 106
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "19ef6d4b1481cb7"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
